{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bit66f7a8720ea44564891eb6b9b39e6c03",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "cuda\n"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0.02424717, -0.04318125, -0.03326246,  0.02928835])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_name = \"CartPole-v0\"\n",
    "env = gym.make(env_name)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.001\n",
    "ENTROPY_BETA = 0.01\n",
    "REWARD_STEPS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(A2C, self).__init__()\n",
    "\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(input_shape[0], 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(input_shape[0], 512), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.policy(x), self.value(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(env, step_size, state, net, gamma, gamma_counter, device=\"cpu\"):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    not_done_idx = []\n",
    "    next_states = []\n",
    "    total_score = 0\n",
    "    \n",
    "    for idx in range(step_size):\n",
    "        state_v = torch.tensor(state, dtype=torch.float).to(device)\n",
    "        policy, _ = net(state_v)\n",
    "        policy = nn.Softmax(dim=-1)(policy)\n",
    "        action = np.random.choice(len(policy), p=policy.detach().numpy())\n",
    "        states.append(np.array(state, copy=False))\n",
    "        actions.append(action) \n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_score += reward\n",
    "        rewards.append(sum(rewards) + reward * (gamma ** gamma_counter))\n",
    "        gamma_counter += 1\n",
    "        if done:\n",
    "            break\n",
    "        else:\n",
    "            not_done_idx.append(idx)\n",
    "            next_states.append(np.array(next_state, copy=False))\n",
    "            state = next_state       \n",
    "        \n",
    "    rewards_np = np.array(rewards, dtype=np.float)\n",
    "    state_tensor = torch.tensor(np.array(states, copy=False), dtype=torch.float).to(device)\n",
    "    actions_tensor = torch.tensor(np.array(actions), dtype=torch.long).to(device)\n",
    "\n",
    "    # * If not_done_idx is not empty\n",
    "    if not_done_idx:\n",
    "        next_states_tensor = torch.tensor(np.array(next_states, copy=False), dtype=torch.float).to(device)\n",
    "        _, next_states_value = net(next_states_tensor)\n",
    "        # * vectorize : np.array([[1],[2],[3]]) ->np.array([1,2,3])\n",
    "        next_states_value_np = next_states_value.data.cpu().numpy()[:, 0]\n",
    "        next_states_value_np *= GAMMA ** step_size\n",
    "        rewards_np[not_done_idx] += next_states_value_np\n",
    "\n",
    "    ref_vals_tensor = torch.tensor(rewards_np, dtype=torch.float).to(device)\n",
    "\n",
    "    return state_tensor, actions_tensor, ref_vals_tensor, total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "A2C(\n  (policy): Sequential(\n    (0): Linear(in_features=4, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=2, bias=True)\n  )\n  (value): Sequential(\n    (0): Linear(in_features=4, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=1, bias=True)\n  )\n)\n"
    }
   ],
   "source": [
    "initial_state = env.reset()\n",
    "observation_space_shape = env.observation_space.shape\n",
    "action_n = env.action_space.n\n",
    "net = A2C(observation_space_shape, action_n)\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Total_reward : 4.344444444444444, loss : 73.058\nTotal_reward : 4.178947368421053, loss : 86.090\nTotal_reward : 4.0793103448275865, loss : 165.532\nTotal_reward : 4.0487179487179485, loss : 173.693\nTotal_reward : 4.010204081632653, loss : 106.093\nTotal_reward : 3.9966101694915253, loss : 359.200\nTotal_reward : 4.01304347826087, loss : 70.939\nTotal_reward : 4.00253164556962, loss : 48.661\nTotal_reward : 3.993258426966292, loss : 289.602\nTotal_reward : 3.995959595959596, loss : 46.000\nTotal_reward : 3.992660550458716, loss : 121.433\nTotal_reward : 3.969747899159664, loss : 47.741\nTotal_reward : 3.954263565891473, loss : 467.948\nTotal_reward : 3.948201438848921, loss : 322.613\nTotal_reward : 3.9436241610738256, loss : 475.851\nTotal_reward : 3.952830188679245, loss : 707.422\nTotal_reward : 3.9485207100591717, loss : 23.301\nTotal_reward : 3.941899441340782, loss : 111.505\nTotal_reward : 3.93015873015873, loss : 171.326\nTotal_reward : 3.928643216080402, loss : 186.543\nTotal_reward : 3.9406698564593303, loss : 164.247\nTotal_reward : 3.9383561643835616, loss : 188.461\nTotal_reward : 3.935807860262009, loss : 104.171\nTotal_reward : 3.9426778242677822, loss : 169.170\nTotal_reward : 3.9377510040160644, loss : 65.633\nTotal_reward : 3.937837837837838, loss : 235.742\nTotal_reward : 3.940520446096654, loss : 345.143\nTotal_reward : 3.9379928315412185, loss : 206.180\nTotal_reward : 3.938062283737024, loss : 344.391\nTotal_reward : 3.9411371237458193, loss : 264.130\nTotal_reward : 3.9423948220064724, loss : 647.857\nTotal_reward : 3.9435736677115987, loss : 137.709\nTotal_reward : 3.9419452887537996, loss : 83.872\nTotal_reward : 3.9460176991150444, loss : 109.117\nTotal_reward : 3.9478510028653293, loss : 64.579\nTotal_reward : 3.9512534818941503, loss : 101.152\nTotal_reward : 3.9479674796747966, loss : 204.894\nTotal_reward : 3.9490765171503956, loss : 169.436\nTotal_reward : 3.949100257069409, loss : 469.354\nTotal_reward : 3.9466165413533836, loss : 87.303\nTotal_reward : 3.944987775061125, loss : 438.402\nTotal_reward : 3.94200477326969, loss : 205.487\nTotal_reward : 3.9407925407925406, loss : 416.275\nTotal_reward : 3.938041002277904, loss : 144.044\nTotal_reward : 3.937639198218263, loss : 514.524\nTotal_reward : 3.937037037037037, loss : 70.990\nTotal_reward : 3.9422174840085287, loss : 245.965\nTotal_reward : 3.9407098121085595, loss : 139.630\nTotal_reward : 3.940081799591002, loss : 416.540\nTotal_reward : 3.94188376753507, loss : 314.799\nTotal_reward : 3.93811394891945, loss : 507.895\nTotal_reward : 3.936801541425819, loss : 192.521\nTotal_reward : 3.939319470699433, loss : 236.958\nTotal_reward : 3.9410018552875696, loss : 61.256\nTotal_reward : 3.939890710382514, loss : 128.998\nTotal_reward : 3.94257602862254, loss : 304.926\nTotal_reward : 3.940421792618629, loss : 439.183\nTotal_reward : 3.9395509499136443, loss : 59.775\nTotal_reward : 3.93955857385399, loss : 356.643\nTotal_reward : 3.9398998330550916, loss : 108.466\nTotal_reward : 3.937766830870279, loss : 491.926\nTotal_reward : 3.9392568659127627, loss : 180.506\nTotal_reward : 3.938314785373609, loss : 416.388\nTotal_reward : 3.93849765258216, loss : 578.085\nTotal_reward : 3.9375963020030817, loss : 228.743\nTotal_reward : 3.9388467374810316, loss : 333.654\nTotal_reward : 3.939162929745889, loss : 210.568\nTotal_reward : 3.939911634756996, loss : 351.956\nTotal_reward : 3.9400580551523947, loss : 78.419\nTotal_reward : 3.939914163090129, loss : 64.462\nTotal_reward : 3.93737658674189, loss : 143.493\nTotal_reward : 3.93769123783032, loss : 36.489\nTotal_reward : 3.939506172839506, loss : 30.797\nTotal_reward : 3.9382949932341003, loss : 42.945\nTotal_reward : 3.9355140186915887, loss : 687.488\nTotal_reward : 3.934387351778656, loss : 120.853\nTotal_reward : 3.933810143042913, loss : 153.105\nTotal_reward : 3.934274711168164, loss : 400.175\nTotal_reward : 3.937769328263625, loss : 72.612\nTotal_reward : 3.939299123904881, loss : 237.023\nTotal_reward : 3.9374536464771324, loss : 119.793\nTotal_reward : 3.9384615384615387, loss : 161.390\nTotal_reward : 3.9370325693606754, loss : 213.744\nTotal_reward : 3.937544696066746, loss : 34.849\nTotal_reward : 3.9372202591283862, loss : 323.361\nTotal_reward : 3.9381839348079164, loss : 98.882\nTotal_reward : 3.9388952819332568, loss : 182.734\nTotal_reward : 3.9382252559726965, loss : 312.307\nTotal_reward : 3.9395950506186725, loss : 130.144\nTotal_reward : 3.9379310344827587, loss : 170.264\nTotal_reward : 3.9364136413641364, loss : 40.000\nTotal_reward : 3.937431991294886, loss : 349.954\nTotal_reward : 3.9368137782561896, loss : 148.993\nTotal_reward : 3.938338658146965, loss : 386.722\nTotal_reward : 3.9365648050579556, loss : 417.880\nTotal_reward : 3.935766423357664, loss : 78.730\nTotal_reward : 3.9366357069143447, loss : 211.422\nTotal_reward : 3.93585291113381, loss : 312.000\nTotal_reward : 3.9332659251769466, loss : 441.223\nTotal_reward : 3.9335335335335335, loss : 17.998\nTotal_reward : 3.9344895936570863, loss : 584.498\nTotal_reward : 3.934838076545633, loss : 633.364\nTotal_reward : 3.9328474246841596, loss : 93.844\nTotal_reward : 3.934456207892204, loss : 148.453\nTotal_reward : 3.9329837940896093, loss : 221.453\nTotal_reward : 3.932766761095373, loss : 40.415\nTotal_reward : 3.9307764265668848, loss : 111.194\nTotal_reward : 3.9296570898980536, loss : 225.608\nTotal_reward : 3.927548209366391, loss : 26.397\nTotal_reward : 3.926842584167425, loss : 171.469\nTotal_reward : 3.927682596934175, loss : 630.358\nTotal_reward : 3.929490616621984, loss : 103.610\nTotal_reward : 3.929406554472985, loss : 198.173\nTotal_reward : 3.928182616330114, loss : 152.707\nTotal_reward : 3.927850304612707, loss : 115.727\nTotal_reward : 3.92648835202761, loss : 30.138\nTotal_reward : 3.9266894781864843, loss : 239.007\nTotal_reward : 3.926632739609839, loss : 17.994\nTotal_reward : 3.927502102607233, loss : 71.794\nTotal_reward : 3.9277731442869057, loss : 594.048\nTotal_reward : 3.9283705541770058, loss : 216.150\nTotal_reward : 3.928219852337982, loss : 620.646\nTotal_reward : 3.9279902359641987, loss : 127.591\nTotal_reward : 3.9281678773204196, loss : 610.430\nTotal_reward : 3.9284227381905525, loss : 477.898\nTotal_reward : 3.9279586973788723, loss : 317.833\nTotal_reward : 3.928053585500394, loss : 185.260\nTotal_reward : 3.928459734167318, loss : 638.990\nTotal_reward : 3.9282389449185415, loss : 177.161\nTotal_reward : 3.9287143956889916, loss : 545.711\nTotal_reward : 3.9275783040488923, loss : 95.213\nTotal_reward : 3.9281273692191054, loss : 555.983\nTotal_reward : 3.927915726109857, loss : 107.484\nTotal_reward : 3.9282300224047795, loss : 580.355\nTotal_reward : 3.9292068198665677, loss : 402.368\nTotal_reward : 3.9287711552612214, loss : 654.622\nTotal_reward : 3.9295836376917457, loss : 836.120\nTotal_reward : 3.930239303843365, loss : 201.904\nTotal_reward : 3.9316774658027356, loss : 847.077\nTotal_reward : 3.9320943531093637, loss : 104.132\nTotal_reward : 3.9326472675656494, loss : 776.870\nTotal_reward : 3.931923890063425, loss : 98.384\nTotal_reward : 3.9313505948215535, loss : 39.062\nTotal_reward : 3.9318276580958997, loss : 147.087\nTotal_reward : 3.932712215320911, loss : 114.108\nTotal_reward : 3.932076764907471, loss : 461.629\nTotal_reward : 3.93287950987066, loss : 400.588\nTotal_reward : 3.932657200811359, loss : 102.847\nTotal_reward : 3.9324378777703157, loss : 162.192\nTotal_reward : 3.9325550366911273, loss : 543.540\nTotal_reward : 3.9337309476474487, loss : 85.862\nTotal_reward : 3.9336405529953917, loss : 144.529\nTotal_reward : 3.934597776324395, loss : 318.430\nTotal_reward : 3.9355425601039635, loss : 211.466\nTotal_reward : 3.935119431891543, loss : 174.524\nTotal_reward : 3.9348300192431047, loss : 303.792\nTotal_reward : 3.935818992989165, loss : 75.130\nTotal_reward : 3.9352754908169727, loss : 399.270\nTotal_reward : 3.9344870988042793, loss : 250.025\nTotal_reward : 3.935397123202001, loss : 604.875\nTotal_reward : 3.9369173399627098, loss : 162.147\nTotal_reward : 3.9354539839407043, loss : 169.908\nTotal_reward : 3.9368937998772253, loss : 301.483\nTotal_reward : 3.9369127516778524, loss : 107.991\nTotal_reward : 3.93650697392359, loss : 101.835\nTotal_reward : 3.934900542495479, loss : 528.761\nTotal_reward : 3.9345715997603357, loss : 31.395\nTotal_reward : 3.935020845741513, loss : 94.974\nTotal_reward : 3.9340438129070456, loss : 380.654\nTotal_reward : 3.934961742201295, loss : 159.581\nTotal_reward : 3.9365125804564074, loss : 488.873\nTotal_reward : 3.9361838278068646, loss : 775.992\nTotal_reward : 3.9362058993637943, loss : 377.712\nTotal_reward : 3.9360552041403105, loss : 294.830\n"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-82b14922ae6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mloss_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentropy_loss_tensor\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_value_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mloss_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gamma_counter = 0\n",
    "total_reward = []\n",
    "print_idx = 0\n",
    "while True:\n",
    "    print_idx += 1\n",
    "    states_v, actions_t, ref_vals_tensor, total_score = step(env, step_size=5, state=initial_state, net=net, gamma=0.99, gamma_counter=gamma_counter)\n",
    "    total_reward.append(total_score)\n",
    "    if len(ref_vals_tensor) == 1:\n",
    "        state = env.reset()\n",
    "        gamma_counter = 0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    logits_tensor, value_tensor = net(states_v)\n",
    "    loss_value_tensor = nn.MSELoss()(value_tensor.squeeze(-1), ref_vals_tensor)\n",
    "\n",
    "    adv_tensor = ref_vals_tensor - value_tensor.detach()\n",
    "    log_prob_tensor = nn.LogSoftmax(dim=1)(logits_tensor)\n",
    "    log_prob_actions_tensor = adv_tensor * log_prob_tensor[:, actions_t]\n",
    "    loss_policy_tensor = -log_prob_actions_tensor.mean()\n",
    "\n",
    "    prob_tensor = nn.Softmax(dim=1)(logits_tensor)\n",
    "    entropy_loss_tensor = ENTROPY_BETA * (prob_tensor * log_prob_tensor).sum(dim=1).mean()\n",
    "\n",
    "    loss_tensor = entropy_loss_tensor + loss_value_tensor\n",
    "    loss_tensor.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_tensor += loss_policy_tensor\n",
    "\n",
    "    if print_idx % 100 == 0:\n",
    "        print(f\"Total_reward : {np.mean(total_reward[10:])}, loss : {loss_tensor.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}