# CH 22 Beyond Model-Free Imagination

## Model-based versus model-free

We distinguished three main aspects:

- Value-based and policy-based
- On-policy and off-pocliy
- Model-Free and model-free

There were enough examples of methods on both sides of the first and second categories, ***but all the methods that we have covered so far were 100% model-free***.

- However, this doesn't mean that model-free methods are more important or better than their model-based counterparts.
- Historically, due to their sample efficiency, ***the model-based methods have been used in the robotics field and other industrial controls***.
  - 로봇 같은 하드웨어는 비용이 크기 때문에 실험이 어려움 -> 게임 같은 걸로 시뮬레이션 많이 한 이유

In the names of both classes, ***"model" means the model of the environment, which could have various forms***, for example, providing us with a new state and reward from the current state and action. 

- The source of the observations and reward was the environment itself, which in some cases could be very slow and inefficient.

In a model-based approach, we're trying to learn the model of the environment to reduce this "real environment" dependency.

- At the high level, the model is some kind of black box
- If we have an accurate environment model, our agent can produce any number of trajectories that it needs, simply by using this model instead of executing the actions in the real world.

***There are two motivations for using the model-based approach as opposed to model-free***.

- The first and the most important one is ***sample efficiency caused by less dependency on the real environment***.
  - Ideally, by having an accurate model, we can avoid touching the real world and use only the trained model.
  - In real applications, it is almost never possible to have the precise model of the environment, but even an imperfect model can significantly reduce the number of samples needed.
- ***The second reason for a model-based approach is the transferability of the environment model across goals***.
  - If you have a good model for a robot manipulator, you can use it for a wide variety of goals without retraining everything from scratch.

## Model imperfections

There is a serious issue with the model-based approach: when our model makes mistakes or is just inaccurate in some parts of the environment, the policy learned from this model may be totally wrong in real-life situations. 

***To deal with this, we have several options***.

- The most obvious option is to make the model better.
  - Unfortunately, this can just mean that we need more observations from the environment, which is what we've tried to avoid.
  - 더 좋은 모델을 만든다는건 시행착오를 더 겪어야 함을 의미
- The more complicated and nonlinear the behavior of the environment, the worse the situation will be for modeling it properly.
  - 복잡하고 비선형인 환경은 모델링 하기가 힘듬

***Another interesting way of looking at environment models is to augment model-free policy with model-based paths***.

- In that case, we're not trying to build the best possible model of the environment, but ***just giving our agent extra information and letting it decide by itself whether the information will be useful during the training or not***.

One of the first steps in that direction was carried out by DeepMind in their system UNREAL, called *Reinforcement Learning with Unsupervised Auxiliary Tasks*, which was published in 2016.

- The authors augmented the asynchronous advantage actor-critic (A3C) agent with extra tasks learned in an unsupervised way during the normal training.
- The main tests of the agent were performed in a partially observable first-person view maze navigation problem

***The novel approach of the paper was in artificially injecting extra auxiliary tasks not related to the usual RL methods' objectives of value or discounted reward***.

- ***An immediate reward prediction***: From the history of observations, the agent was asked to predict the immediate reward of the current step
- ***Pixel control***: The agent was asked to communicate with the environment to maximize the change in its view
- ***Feature control***: The agent was learning how to change specific features in its internal representation

Those tasks were not directly related to the agent's main objective of maximizing the total reward, ***but they allowed the agent to get a better representation of low-level features*** and allowed UNREAL to get better results.

## The imagination-augmented agent(I2A)

The overall idea of the new architecture, called imagination-augmented agent (I2A), is ***to allow the agent to imagine future trajectories using the current observations and incorporate these imagined paths into its decision process***. 

The agent consists of two different paths used to transform the input observation: ***model-free and imagination***.

- Model-free is a standard set of convolution layers that transforms the input image in high-level features.
- The other path, imagination, consists of a set of trajectories imagined from the current observation.
  - ***The trajectories are called rollouts and they are produced for every available action in the environment***.
  - Every rollout consists of a fixed number of steps into the future, and on every step, a special model, called the ***environment model*** (EM) (but not to be confused with the expectation maximization method), produces the next observation and predicted immediate reward from the current observation and the action to be taken.
  - ***Every rollout for every action is produced by taking the current observation into the EM and then feeding the predicted observation to the EM again N times***.
  - All the steps from the single rollout are passed to another network, called the rollout encoder, which encodes them into a fixed-size vector.
  - For every rollout, we get those vectors, concatenate them together, and feed them to the head of the agent, which produces the usual policy and value estimations for the A3C algorithm.

### The EM(Environment Model)

The goal of the EM is to convert the current observation and the action into the next observation and the immediate reward.

### The rollout policy

During the rollout steps, we need to make decisions about the action to be taken during our imagined trajectory.

- As mentioned, the action for the first step is set explicitly, as we produce an individual rollout trajectory for every action we have, but the subsequent steps require somebody to make this decision.
  - Ideally, we would like those actions to be similar to our agent's policy, but we can't just ask our agent to produce the probabilities, as it will require rollouts to be created in the imagination path.
- ***To break this tie, a separate rollout policy network is trained to produce similar output to our main agent's policy***.
  - The rollout policy is a small network, with a similar architecture to A3C, that is trained in parallel to the main I2A network ***using a cross-entropy loss between the rollout policy network output and the output of the main network***.
  - In the paper, this training process is called "policy distillation."

### The rollout encoder

The final component of the I2A model is the rollout encoder, which takes rollout steps (observation and reward pairs) as input and produces the fixed-sized vector, which embeds the information about the rollout.

