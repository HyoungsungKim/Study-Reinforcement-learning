# CH13 Asynchronous Advantage Actor-Critic

We will take a look at two approaches for adding asynchronous behavior to the basic A2C method: data-level and gradient-level parallelism. 

## Correlation and sample efficiency

One of the approaches to improving the stability of the policy gradient family of methods is using multiple environments in parallel.

In the case of policy gradient methods, each experience obtained from the environment can be used only once, as our method requires fresh data, ***so the data efficiency of policy gradient methods could be an order of magnitude lower than the value-based, off-policy methods.***

- If your environment is cheap in terms of the agent interaction (the environment is fast, has a low memory footprint, allows parallelization, and so on), policy gradient methods could be a better choice.
- On the other hand, if the environment is expensive and obtaining a large amount of experience could slow down the training process, the value-based methods could be a smarter way to go.

## Adding an extra A to A2C

With regard to actor-critic parallelization, two approaches exist:

1. ***Data parallelism***: We can have several processes, each of them communicating with one or more environments, and providing us with the training transitions(s, r, a, s'). ***All those samples are gathered together in one single training process, which calculates losses and performs an SGD update***. Then, the updated neural network (NN) parameters need to be broadcast to all other processes to uses in future environment communications
2. ***Gradients parallelism***: As the goal of the training process is the calculation of gradients to update our NN, we can have several processes calculating gradients on their own training samples. Then, ***these gradients can be summed together to perform the SGD update in one process***. Of course, updated NN weights also need to be propagated to all workers to keep data on-policy

- The difference between the two methods might not look very significant from the diagrams, ***but you need to be aware of the computation cost***.
- The heaviest operation in A2C optimization is the training process, which consists of loss calculation from data samples (forward pass) and the calculation of gradients with respect to this loss.
- The SGD optimization step is quite lightweight: basically, just adding the scaled gradients to the NN's weights.
- ***By moving the computation of loss and gradients in the second approach from the central process, we eliminated the major potential bottleneck and made the whole process significantly more scalable.***
  - Forwarding과 gradient calculation은 비용이 많이 드는 계산임. 두번째 방법에서 이 연산들을 parallel하게 진행해서 연산장치에 걸리는 부담을 감소시킴.

In practice, the choice of the method mainly depends on your resources and your goals.

- If you have one single optimization problem and lots of distributed computation resources, such as a couple of dozen graphics processing units (GPUs) spread over several machines in the networks, then gradients parallelism will be the best approach to speed up your training.
- However, in the case of one single GPU, both methods will provide a similar performance, but the first approach is generally simpler to implement, as you don't need to mess with low-level gradient values. 

In this chapter, we will implement both methods on our favorite Pong game to see the difference between the approaches and look at PyTorch multiprocessing capabilities.