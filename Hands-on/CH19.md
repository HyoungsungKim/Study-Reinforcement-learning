# CH 19 Trust Regions - PPO, TRPO, ACKTR, and SAC

we will take a look at the approaches used to improve the stability of the stochastic policy gradient method.

- Proximal policy optimization (PPO)
- Trust region policy optimization(TRPO)
- Advantage actor-critic(A2C) using Kronecker-factored trust region(ACKTR)

The overall motivation of the methods that we will look at is to improve the stability of the policy update during the training. ***There is a dilemma***:

- On the one hand, we'd like to train as fast as we can, ***making large steps during the stochastic gradient descent (SGD) update***.
- On the other hand, a large update of the policy is usually a bad idea. ***The policy is a very nonlinear thing, so a large update can ruin the policy we've just learned***.

Things can become even worse in the reinforcement learning (RL) landscape, as making a bad update of the policy once won't be recovered from by subsequent updates.

- Instead, the bad policy will bring us bad experience samples that we will use on subsequent training steps, which could break our policy completely. Thus, we want to avoid making large updates by all means possible.
  - One of the naïve solutions would be to use a small learning rate to take baby steps during the SGD, but this would significantly slow down the convergence
- To break this vicious cycle, several attempts have been made by researchers to estimate the effect that our policy update is going to have in terms of the future outcome.
  - ***One of the popular approaches is trust region optimization extension***, which constrains the step taken during the optimization to limit its effect on the policy. 

## PPO

The core improvement over the classic A2C method is changing the formula used to estimate the policy gradients. Instead of using the gradient of logarithm probability of the action taken, ***the PPO method uses a different objective***:

- The ratio between the new and the old policy scaled by the advantages.

In math form, the old A2C object could be written as 
$$
J_\theta = E_t[\nabla_\theta \log{\pi_\theta(a_t|s_t)A_t}]
$$
The new objective proposed by PPO is 
$$
J_\theta = E_t[\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} A_t]
$$

- However, if we just start to blindly maximize this value, it may lead to a very large update to the policy weights.
- To limit the update, the clipped objective is used.

$$
\begin{align}
r_t(\theta) & = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \\
J^{clip}_\theta = \mathbb{E}_t[min(r_t(\theta)A_t,\text{ }clip(r_t(\theta), 1-\epsilon, 1+\epsilon)A_t]
\end{align}
$$

- This objective limits the ratio between the old and the new policy to be in the interval $$[1-\epsilon, 1+\epsilon]$$, so by varying $$\epsilon$$, we can limit the size of the update
- Another difference from the A2C methods is the way that we estimate the advantage
- In the PPO paper, the authors used a more general  estimation in the form

$$
A_t = \sigma_t + (\gamma \lambda)\sigma_{t+1} + (\gamma \lambda)^2\sigma_{t+2} + ... + (\gamma \lambda)^{T-t+1}\sigma_{t-1}
$$

- $$\sigma_t = r_t + \gamma V(s_{t+1}) - V(s_t)$$
  - It means Q - V
- The PPO methods also uses a slightly different training procedure: a long sequence of samples is obtained from the environment and then the advantage is estimated for the whole sequence before several epochs of training are performed.

### GAE

#### Multi-step returns

- 1 step : returns low variance but high biased
- Infinity step : return high variance but low biased

Another method to trade off between the bias and variance of the estimator is to use $$\lambda$$-return, calculated as an exponentially-weighted average of n-step returns with decay parameter $$\lambda$$
$$
R_t(\lambda) = (1-\lambda)\sum^{\infty}_{n=1}\lambda^{n-1}R_t^n
$$

- Estimating the advantage with $$\lambda$$-return, yields the generalized advantage estimator $$GAE(\lambda)$$

$$
\hat{\mathcal{A}}_t = R_t(\lambda) - V(s_t)
$$

- 람다가 1이면 몬테카를로, 0이면 1 step learning임. 내 생각에는 람다가 0.95이면 에피소드의 95퍼센트까지 보고 없데이트를 하고, t가 커질수록 람다가 0으로 수렴하면서 1 step learning이 됨

## SAC

At the moment, it's considered to be one of the best methods for continuous control problems.



