# Ch11 Policy Gradient - an Alternative

- The central topic in value iteration and Q-learning is the value of the state (V) or value of the state
  and action (Q). 

## Value and policy

### Why policy?

- First of all, the policy is what we are looking for when we are solving a reinforcement learning (RL) problem.
- ***When you don't care much about the value,*** because you need to make the decision on what to do fast and that's it.
  - Our Q-learning approach tried to answer the policy question indirectly by approximating the values of the states and trying to choose the best alternative, but if we are not interested in values, why do extra work?
- Another reason why policies may be more attractive than values is due to environments with lots of actions or, in the extreme case, with a ***continuous action space.***
  - To be able to decide on the best action to take having Q(s, a), we need to solve a small optimization problem, finding a, which maximizes Q(s, a).
  - It is not easy in continuous area
- An extra benefit of policy learning is an environment with ***stochasticity.***

### Policy representation

So how do we represent the policy?

- In the case of Q-values, they were parametrized by the NN that returns values of actions as scalars.

If we want our network to parametrize the actions, we have several options.

- The first and the simplest way could be just returning the identifier of the action (in the case of a discrete set of actions).
  - However, this is not the best way to deal with a discrete set.
- ***A much more common solution, which is heavily used in classification tasks, is to return the probability distribution of our actions.***
  - In other words, for N mutually exclusive actions, we return N numbers representing the probability of taking each action in the given state (which we pass as an input to the network)
  - DQN에서 사용했던 방법

### Policy gradient

We solved a very similar problem using the cross-entropy method: our network took observations as inputs and returned the probability distribution of the actions.

- In fact, the cross-entropy method is a younger brother of the methods that we will discuss in this part of the book.
- To start, we will get acquainted(정통한) with the method called REINFORCE, which has only minor differences from the cross-entropy method

Policy gradient
$$
\nabla J \approx \mathbb{E}[Q(s,a) \nabla \log{\pi(a|s)}]
$$

- Q(s,a) : Value when s and a are given (It is function, not probability)
- $$\pi(a|s)$$ : Probability of action `a` when state `s` is given
- ***The policy gradient defines the direction in which we need to change our network's parameters*** to improve the policy in terms of the accumulated total reward.
  - The scale of the gradient is proportional to the value of the action taken, which is Q(s, a) in the formula.
  - And the gradient itself is equal to the gradient of log probability of the action taken.
    - Scale of policy gradient way : proportional to Q(s,a)
    - Gradient of policy gradient way : equal to the gradient of log probability of the action taken
  - This means that we are trying to increase the probability of actions that have given us good total reward and decrease the probability of actions with bad final outcomes.
  - Expectation, $$\mathbb{E}$$ in the formula, just means that we average the gradient of several steps that we have taken in the environment.

From a practical point of view, policy gradient methods could be implemented by performing optimization of this loss function
$$
\mathcal{L} = -Q(s,a)\log{\pi(a|s)}
$$

- $$\pi(a|s)$$ is less than 1. Therefore result of log is concave shape
- So multiplying minus 1, we can get convex shape
- As a result, minimizing loss using SGD means maximizing policy gradient

## The REINFORCE method

Training procedure is a policy gradient method with Q(s, a) = 1 for state and action pairs from good episodes (with a large total reward) and Q(s, a) = 0 for state and action pairs from worse episodes.

- The cross-entropy method worked even with those simple assumptions, but the obvious improvement will be to use Q(s, a) for training instead of just 0 and 1.
  - Why should it help?
- ***The answer is a more fine-grained separation of episodes.***
  - For example, transitions of the episode with the total reward of 10 should contribute to the gradient more than transitions from the episode with the reward of 1.
- The second reason to use Q(s, a) instead of just 0 or 1 constants is ***to increase probabilities of good actions in the beginning of the episode and decrease the actions closer to the end of the episode*** (as Q(s, a) incorporates the discount factor, uncertainty for longer sequences of actions is automatically taken into account). 
- That's exactly the idea of the method called REINFORCE. 

REINFORCE steps

1. Initialize network with random weights
2. Play N full episode, saving their (s,a,r,s') transitions
3. For every step, `t`, of every episode, `k`, calculate the discounted total reward for subsequent steps:
   - $$Q_{k,t} = \sum_{i=0} \gamma^ir_i$$
4. Calculate the loss function for all transitions
   - $$\mathcal{L} = - \sum_{k,t}Q_{k,t}\log(\pi(s_{k,t}, a_{k,t}))$$
5. Perform an SGD update of weights, minimizing the loss
6. Repeat from step 2 until converged

###  Difference with Q-learning

- ***No explicit exploration is needed.*** In Q-learning, we used an epsilon-greedy strategy to explore the environment and prevent our agent from getting stuck with a non-optimal policy.
  - ***Now, with probabilities returned by the network, the exploration is performed automatically.*** In the beginning, the network is initialized with random weights, and it returns a uniform probability distribution.
  - This distribution corresponds to random agent behavior.
- ***No replay buffer is used.***
  - ***Policy gradient methods belong to the on-policy methods class,*** which means that we can't train on data obtained from the old policy. This is both good and bad. The good part is that such methods usually converge faster. The bad side is that they usually require much more interaction with the environment than off-policy methods such as DQN.
- ***No target network is needed.*** Here, we use Q-values, but they are obtained from our experience in the environment. In DQN, we used the target network to break the correlation in Q-values approximation, but we are not approximating anymore.
  - In the next chapter, you will see that the target network trick can still be useful in policy gradient methods.

### Policy-based VS Value-based methods

- ***Policy methods directly optimize*** what we care about: our behavior.
  - Value methods, such as DQN, do the same indirectly, learning the value first and providing us with the policy based on this value.
- ***Policy methods are on-policy and require fresh samples from the environment.***
  - Value methods can benefit from old data, obtained from the old policy, human demonstration, and other sources.
- ***Policy methods are usually less sample-efficient, which means they require more interaction with the environment.*** 
  - Value methods can benefit from large replay buffers. However, sample efficiency doesn't mean that value methods are more computationally efficient, and very often, it's the opposite.
- In the preceding example, during the training, we needed to access our NN only once, to get the probabilities of actions.
  - In DQN, we need to process two batches of states: one for the current state and another for the next state
    in the Bellman update.(When calculate value)

## REINFORCE issues

Unfortunately, both REINFORCE and the cross-entropy method still suffer from several problems, which make both of them limited to simple environments.

### Full episodes are required

The purpose of the complete episode requirement is to get as accurate a Q-estimation as possible. When we talked about DQN, you saw that, in practice, it's fine to replace the exact value for a discounted reward with our estimation using the one-step Bellman equation: $$Q(s,a) = r_a + \gamma V(s')$$.

- To estimate V(s), we used our own Q-estimation, ***but in the case of the policy gradient, we don't have `V(s)` or `Q(s,a)` anymore***
- On the one hand, we can ask our network to estimate V(s) and use this estimation to obtain Q. This approach is called the actor-critic method, which is the most popular method from the policy gradient family.

### High gradient variance

The range of reward is heavily environment-dependent. ***one lucky episode will dominate in the final gradient.***

In mathematical terms, ***the policy gradient has high variance,*** and we need to do something about this in complex environments; otherwise, ***the training process can become unstable.*** The usual approach to handling this is subtracting a value called the baseline from the Q. The possible choices for the baseline are as follows:

- Some constant value, which is normally the mean of the discounted rewards
- The moving average of the discounted rewards
- The value of the state, V(s)

### Exploration

Even with the policy represented as the probability distribution, there is a high chance that the agent will converge to some locally optimal policy and stop exploring the environment.

- In DQN, we solved this using epsilon-greedy action selection
- Policy gradient methods allow us to follow a better path, called the ***entropy bonus.***

Entropy of policy
$$
H(\pi) =  - \sum \pi(a|s) \log{\pi(a|s)}
$$

- To prevent our agent from being stuck in the local minimum, ***we subtract the entropy from the loss function,*** punishing the agent for being too certain about the action to take.

### Correlation between samples

Training samples in a single episode are usually heavily correlated, which is bad for SGD training.

- In the case of DQN, we solved this issue by having a large replay buffer
- ***To solve this in policy gradient, parallel environments are normally used.*** The idea is simple: instead of communicating with one environment, we use several and use their transitions as training data.