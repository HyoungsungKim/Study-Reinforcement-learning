# CH 21 Advanced Exploration

## What is wrong witrh epsilon-greedy?

Recent research shows that this approach is very far from being ideal

- Samples that added in buffer with high epsilon is not good data to learn
- 또한 무작위로 선택한 action이 충분한 exploration을 보장하지 않음

## Alternative ways of exploration

- ***Randomness in the policy***, when stochasticity is added to the policy that we use to get samples.
  - The method in this family is noisy networks, which we have already covered.
- ***Count-based methods***, which keep track of the count of times the agent has seen the particular state.
  - We will check two methods: ***the direct counting of states and the pseudo-count method***.
- ***Prediction-based methods***, which try to predict something from the state and from the quality of the prediction. We can make judgements about the familiarity of the agent with this state.
  - To illustrate this approach, we will take a look at the policy distillation method, which has shown state-of-the-art results on hard-exploration Atari games like Montezuma's Revenge.

## Noisy networks

The idea is to add Gaussian noise to the network's weights and learn the noise parameters (mean and variance) using backpropagation, in the same way that we learn the model's weights.

- The difference with dqn is noisy network apply stochasticity to the network.
  - In epsilon-greedy, randomness is added to the actions.
  - In noisy networks, randomness is injected into part of the network itself, which means adding stochasicity to our current policy.
  - In addition, parameters of the noise might be learned during the training, so the training process might increase or decrease this policy randomness if needed.

## Count-based methods

This family of methods is based on the intuition to visit states that were not explored before.

- In simple cases, ***when the state space is not very large and different states are easily distinguishable from each other, we just count the number of times we have seen*** the state or state + action and prefer to get to the states for which this count is low.
- ***This could be implemented as an intrinsic reward*** that is added to the reward obtained from the environment (which is called extrinsic reward in this context).
  - One of the options to formulate such a reward is to use the ***bandits exploration*** approach

## Prediction-based methods

The third family of exploration methods is based on another idea of predicting something from the environment data.

- If the agent can make accurate predictions, it means the agent has been in this situation enough and it isn't worth exploring it.
- But if something unusual happens and our prediction is significantly off, it might mean that we need to pay attention to the state that we're currently in. 