# Ch 17 Continuous Action Space

Continuous action space problems are an important subfield of RL, both theoretically and practically, because they have essential applications in robotics (which will be the subject of the next chapter), control problems, and other fields in which we communicate with physical objects.

## Why a continuous space?

In fact, when you need to communicate with a physical world, a continuous action space is much more likely than having a discrete set of actions.

## The action space

### A2C method

Returning to our action representation options, if you remember from Chapter 11, Policy Gradients â€“ an Alternative, the representation of an action as a concrete value has different disadvantages, mostly related to the exploration of the environment. ***A much better choice will be something stochastic***. The simplest alternative will be the network returning parameters of the Gaussian distribution. For N actions, this will be the network returning parameters of Gaussian distribution.

### Deterministic policy gradients

The next method that we will take a look at is called deterministic policy gradients, ***which is an actor-critic method but has a very nice property of being off-policy***. 

***In A2C method, the actor estimates the stochastic policy***, which returns the probability distribution over discrete actions or, as we have just covered in the previous section, the parameters of normal distribution. In both cases, our policy is stochastic, so, in other words, ***our action taken is sampled from this distribution***.

***Deterministic policy gradients also belong to the A2C family, but the policy is deterministic***, which means that it directly provides us with the action to take from the state.

- This makes it possible to apply the chain rule to the Q-value, and by maximizing the Q, the policy will be improved as well.
- David Silver proved that the stochastic policy gradient is equivalent to the deterministic policy gradient.

Note that, despite both the A2C and deep deterministic policy gradients (DDPG) methods belonging to the A2C family, the way that the critic is used is different.

- In A2C, we use the critic as a baseline for a reward from the experienced trajectories, so the critic is an optional piece (without it, we will get the REINFORCE method) and is used to improve the stability.
- In DDPG, the critic is used in a different way. As our policy is deterministic, we can now calculate the gradients from Q, which is obtained from the critic network, which uses actions produced by the actor, so the whole system is differentiable and could be optimized end to end with stochastic gradient descent (SGD).
  - To update the critic network, we can use the Bellman equation to find the approximation of Q(s, a) and minimize the MSE objective.
  - The critic is updated as we did in A2C, and the actor is updated in a way to maximize the critic's output.

#### Exploration

We can do this by adding noise to the actions returned by the actor before we pass them to the environment. There
are several options here. The simplest method is just to add the random noise to the $$\mu(s) + \epsilon \mathcal{N}$$ actions.

- A fancier approach to the exploration is to use the previously mentioned stochastic model, which is very popular in the financial world and other domains dealing with stochastic processes (OU processes).

### Distributed policy gradient

The full name of the method is ***distributed distributional deep deterministic policy gradients*** or D4PG for short. 

- In short, rainbow dqn with dpg
- DDPG used the OU process for exploration, but according to the D4PG authors, they tried both OU and adding simple random noise to the actions, and the result was the same.