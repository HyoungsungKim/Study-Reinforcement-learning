# Ch12 The Actor-Critic Method

REINFORCE and policy gradient are worked well in small environmnet, however these are not worked in complicated environment like Atari-pong

## Variance reduction

In statistics, variance is the expected square deviation of a random variable from the expected value of that variable.
$$
Va[x] = \mathbb{E}[(x-\mathbb{E}[x])^2]
$$

- Variance shows us how far values are dispersed from the mean.

$$
\nabla J \approx \mathbb{E}[Q(s,a)\nabla \log{\pi(a|s)}]
$$

- Q(s,a) : Scaling factor -> Specifies how much we want to increase or decrease the probability of the action taken in the particular state.
- In an attempt to increase REINFORCE stability, ***we subtracted the mean reward from the gradient scale.***
- To overcome this in the previous chapter, we subtracted the mean total reward from the Q-value and called this mean baseline

## Actor-Critic

The next step in reducing the variance is making our baseline state-dependent.
$$
A(s,a) = Q(s,a) - V(s)
$$

- We will use V(s) as baseline
- The only problem here is that we don't know the value, V(s), of the state that we need to subtract from the discounted total reward, Q(s, a).
- To solve this, let's use another neural network, which will approximate V(s) for every observation.

***When we know the value for any state (or at least have some approximation of it), we can use it to calculate the policy gradient and update our policy network*** to increase probabilities for actions with good advantage values and decrease the chance of actions with bad advantage values.

- ***The policy network (which returns a probability distribution of actions) is called the actor***, as it tells us what to do.
- ***Another network is called critic, as it allows us to understand how good our actions were***.
- This improvement is known under a separate name, the advantage actor-critic method, which is often abbreviated to A2C. 

In practice, the policy and value networks partially overlap, mostly due to efficiency and convergence considerations.

- An entropy bonus is usually added to improve exploration. It's typically written as an entropy value added to the loss function:

$$
\mathcal{L} _H = \beta\sum_i \pi_\theta (s_i) \log{\pi_\theta(s_i)}
$$

- This function has a minimum when the probability distribution is uniform, so by adding it to the loss function, we push our agent away from being too certain about its actions.