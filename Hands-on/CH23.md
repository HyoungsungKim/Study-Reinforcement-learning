# CH 23 AlphaGo Zero

The simplest approach to evaluation is to iterate over the possible actions and recursively evaluate the position after the action is taken. Eventually, this process will lead us to the final position, when no more moves are possible

- By propagating the game result back, we can estimate the expected value of any action in any position.
  - ***One possible variation of this method is called minimax***, which is when we are trying to make the strongest move, but our opponent is trying to make the worst move for us, so we are iteratively minimizing and maximizing the final game objective of walking down the tree of game states.
  - Unfortunately, this brute-force approach doesn't work even for medium-complexity games, as the number of configurations grows exponentially.

## AlphaGo Zero method

- ***Monte Carlo Tree Search(MCTS)*** : The core idea of which is to semi-randomly walk down the game states, expanding them and gathering statistics about the frequency of moves and underlying game outcomes.
- At every moment, we have a ***best player***, which is the model used to generate the data via self-play.
  - Self-play means that the same current best model is used on both sides of the board.
  - This might not look very useful, as having the same model play against itself has an approximately 50% chance outcome, but that's actually what we need: samples of the games where our best model can demonstrate its best skills.
- The third component in the method is the training process of the other, ***apprentice model***, which is trained on the data gathered by the best model during self-play.

### MCTS(Monte Carlo Tree Search)

The number of possible actions at some particular game state is called the ***branching factor***, and it shows the bushiness of the game tree.

- In a general MCTS, we perform many iterations of depth-first search, starting at the current game state and either selecting the actions randomly or with some strategy, which should include enough randomness in its decisions.
  - This process is similar to the value iteration method, when we played the episodes and ***the final step of the episode influenced the value estimation of all the previous steps***.
- This is a general MCTS, and there are many variants of this method related to expansion strategy, branch selection policy, and other details.

In AlphaGo Zero, a variant of MCTS is used. ***For every edge (representing the move from some position), this set of statistics is stored***:

- A prior probability, $$P(s, a)$$, of the edge
- A visit count, $$N(s, a)$$
- An action value, $$Q(s, a)$$.

Each search starts from the root state following the most promising actions, selected using the utility value, $$U(s,a)$$
$$
U(s,a) \propto Q(s,a) + \frac{P(s,a)}{1 + N(s,a)}
$$

- Randomness is added to the selection process to ensure enough exploration of the game tree.

Every search could end up with two outcomes:

- The end state of the game is reached
- Or we face a state that hasn't been explored yet (in other words, has no statistics for values).
  - In this case, the policy neural network (NN) is used to obtain the prior probabilities and the value of the state estimation, and the new tree node with $$N(s, a) = 0, P(s, a) = p_{net}$$ (which is a probability of the move returned by the network) and $$Q(s, a) = 0$$ is created. Besides the prior probability of the actions, the network returns the estimation of the game's outcome (or the value of the state) as seen from the current player.

As we have obtained the value (by reaching the final game state or by expanding the node using the NN), a process called the ***backup of value*** is performed. 

- During the process, we traverse the game path and update statistics for every visited intermediate node;

## The Connect 4 bot



