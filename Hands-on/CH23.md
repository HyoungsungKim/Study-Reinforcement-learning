# CH 23 AlphaGo Zero

The simplest approach to evaluation is to iterate over the possible actions and recursively evaluate the position after the action is taken. Eventually, this process will lead us to the final position, when no more moves are possible

- By propagating the game result back, we can estimate the expected value of any action in any position.
  - ***One possible variation of this method is called minimax***, which is when we are trying to make the strongest move, but our opponent is trying to make the worst move for us, so we are iteratively minimizing and maximizing the final game objective of walking down the tree of game states.
  - Unfortunately, this brute-force approach doesn't work even for medium-complexity games, as the number of configurations grows exponentially.

## AlphaGo Zero method

- ***Monte Carlo Tree Search(MCTS)*** : The core idea of which is to semi-randomly walk down the game states, expanding them and gathering statistics about the frequency of moves and underlying game outcomes.
- At every moment, we have a ***best player***, which is the model used to generate the data via self-play.
  - Self-play means that the same current best model is used on both sides of the board.
  - This might not look very useful, as having the same model play against itself has an approximately 50% chance outcome, but that's actually what we need: samples of the games where our best model can demonstrate its best skills.
- The third component in the method is the training process of the other, ***apprentice model***, which is trained on the data gathered by the best model during self-play.

### MCTS(Monte Carlo Tree Search)

The number of possible actions at some particular game state is called the ***branching factor***, and it shows the bushiness of the game tree.

- In a general MCTS, we perform many iterations of depth-first search, starting at the current game state and either selecting the actions randomly or with some strategy, which should include enough randomness in its decisions.
  - This process is similar to the value iteration method, when we played the episodes and ***the final step of the episode influenced the value estimation of all the previous steps***.
- This is a general MCTS, and there are many variants of this method related to expansion strategy, branch selection policy, and other details.

In AlphaGo Zero, a variant of MCTS is used. ***For every edge (representing the move from some position), this set of statistics is stored***:

- A prior probability, $$P(s, a)$$, of the edge
- A visit count, $$N(s, a)$$
- An action value, $$Q(s, a)$$.

Each search starts from the root state following the most promising actions, selected using the utility value, $$U(s,a)$$
$$
U(s,a) \propto Q(s,a) + \frac{P(s,a)}{1 + N(s,a)}
$$

- Randomness is added to the selection process to ensure enough exploration of the game tree.

Every search could end up with two outcomes:

- The end state of the game is reached
- Or we face a state that hasn't been explored yet (in other words, has no statistics for values).
  - In this case, the policy neural network (NN) is used to obtain the prior probabilities and the value of the state estimation, and the new tree node with $$N(s, a) = 0, P(s, a) = p_{net}$$ (which is a probability of the move returned by the network) and $$Q(s, a) = 0$$ is created. Besides the prior probability of the actions, the network returns the estimation of the game's outcome (or the value of the state) as seen from the current player.

As we have obtained the value (by reaching the final game state or by expanding the node using the NN), a process called the ***backup of value*** is performed. 

- During the process, we traverse the game path and update statistics for every visited intermediate node;

### Progressive strategies for Monte carlo tree search

This method uses two main strategies, which aim at different purposes described below.

1.  A ***selection strategy***, derived from the Multi-Armed Bandit problem, is able to increase the quality of the chosen moves in the tree when the number of simulations grows. Yet, ***the strategy requires the results of several previous simulations***.
2. When a sufficient amount of results is not available, a ***simulation strategy*** decides on the moves to be played.

So, the challenge is: how do we harmonize a simulation strategy (necessary to be applied by a lack of sufficient results) with the selection strategy?

#### Monte-carlo Tree Search

Monte-Carlo Tree Search (MCTS) is a best-first search method, which does not require a positional evaluation function. It is based on a randomized exploration of the search space: in the beginning of the search, exploration is performed fully at random.

- Then, using the results of previous explorations, the algorithm becomes able to predict the most promising moves more accurately, and thus, their evaluation becomes more precise

##### Structure of MCTS

In MCTS, each node `i` represents a given position (also called a state) of the game. A node contains at least the following two pieces of information:

1. The current value $$v_i$$ of the position (usually the average of the results of the simulated games that visited this node)
2. The visit count of this position $$n_i$$. ***MCTS usually starts with a tree containing only the root node***.

MCTS consists of four steps, repeated as long as there is time left.

1. The tree is traversed from the root node to a leaf node (L), using a ***selection strategy***.
2. An ***expansion strategy*** is called to store one (or more) children of `L` in the tree.
3. A ***simulation strategy*** plays moves in self-play until the end of the game is reached.
   - The result R of this “simulated” game is `+1` in case of a win for Black (the first player in Go)
   - `0` in case of a draw
   - And `−1` in case of a win for White. 
4. R is propagated back through the tree according to a ***backpropagation strategy***. 

Finally, the move played by the program is the child of the root with the ***highest visit count***.

##### The four strategic tasks

##### Selection

Selection is the strategic task that selects one of the children of a given node.

- It controls the balance between exploitation and exploration.
  - Exploitation is the task to select the move that leads to the best results so far.
  - Exploration deals with less promising moves that still have to be examined, due to the uncertainty of the evaluation.
- Similar balancing of exploitation and exploration has been studied in the literature, in particular with respect to the Multi-Armed Bandit (MAB) problem.
- The selection problem of MCTS could be viewed as a MAB problem for a given node: the problem is to select the next move (arm) to play, which will give an unpredictable reward (the outcome of a single random game).
- Knowing the past results, the problem is to find the optimal move. ***However, the main difference with the MAB problem is that MCTS works by using sequentially several selections***: the selection at the root node, the selection at depth one, the selection at depth two, etc. 

###### Selection strategy used in MANGO(GO program)

We use the strategy UCT (Upper Confidence Bound applied to Trees).
$$
k \in \underset{i \in I}{argmax} \Bigl( v_i + C \sqrt{\frac{\ln{n_p}}{n_i}} \Bigr)
$$

- Let `I` be the set of nodes rechable from the current node `p`.
- UST select the child `k` of the node `p` that satisfied formula above
- $$v_i$$ : Value of the node `i`
- $$n_i$$ : Visit count of position `i`
- $$n_p$$ : Visit count of position `p`
- `C` : C is a coefficient, which has to be tuned experimentally.
  - MANGO used 0.7
- In practice, UCT is only applied in nodes of which the visit count is higher than a certain threshold `T`

If the node has been visited fewer times than this threshold, the next node is selected according to the ***simulation strategy***

##### Expansion

Expansion is the strategic task that, for a given leaf node `L`, decides whether this node will be expanded by storing one or more of its children in memory. The simplest rule is to expand one node per simulated game.

###### Expansion strategy used in MANGO

In addition to expanding one node per simulated game, we also expand all the children of a node when a node’s visit count equals `T`.

##### Simulation(Also called playout)

***Simulation (also called playout)*** is the strategic task that selects moves in self-play until the end of the game. 

- Indeed,the use of an adequate simulation strategy has been shown to improve the level ofplay significantly.

$$
p_j = \frac{U_j}{\sum_{k \in \mathcal{M}}U_k}
$$

- $$\mathcal{M}$$ : Set of all possible moves for a given position
- Each move $$j \in \mathcal{M}$$ is given an urgency $$U_j > 1$$
- The simulation strategy selects one move from $${M}$$
- ***The probability of each move*** to be selected is $$p_j = \frac{U_j}{\sum_{k \in \mathcal{M}}U_k}$$

##### Backpropagation

Backpropagation is the procedure that propagates the resul tof a simulated game `k` backwards from leaf node `L` to the nodes it had to traverse to reach this leaf node.

## The Connect 4 bot

### The game model

The compactness of the game state representation could have a huge impact on memory requirements and the performance of our training process. However, the game state representation has to be convenient to work with, for
example, when checking the board for a winning position, making a move, and finding all the valid moves from some state.

- State를 가지고 있으면 많은 메모리를 사용 함
- 하지만 state가 많아야 좋은 성능을 얻을 수 있음



