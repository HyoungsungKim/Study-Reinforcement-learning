# CH 14 Training Chatbots with RL

## The deep NLP basics

### RNNs

As an RNN has two inputs, it can be applied to input sequences of any length, just by passing the hidden state produced by the previous entry to the next one.

- The feed-forward NNs are determined by their input and always produce the same output for some fixed input (in testing mode, of course, and not during the training).
- ***An RNN's output depends not only on the input but on the hidden state, which could be changed by the NN itself***.
- So, the NN could pass some information from the beginning of the sequence to the end and produce different output for the same input in different contexts. 

### Word embedding

Another standard building block of modern DL-driven NLP is word embedding, which is also called word2vec by one of the most popular training methods. 

- Normally, NNs work with fixed-sized vectors of numbers, but in NLP, we normally have words or characters as input to the model.
- One possible solution might be one-hot encoding our dictionary, which is when every word has its own position in the input vector and we set this number to 1 when we encounter this word in the input sequence. 
- Unfortunately, one-hot encoding doesn't work very well for several reasons. 
  - ***First of all, our input set is usually not small.***
    - If we want to encode only the most commonly used English dictionary, it will contain at least several thousand words.
  - ***The second problem related to the one-hot representation of words is the uneven frequency of vocabulary***.
    - There are relatively small sets of very frequent words, like a and cat, but a very large set of much more rarely used words, like covfefe or bibliopole, and those rare words can occur only once or twice in a very large text corpus. So, our one-hot representation is very inefficient in terms of space.
  - ***Another issue with simple one-hot representation is not capturing a word's relations.***
- ***To overcome all this, we can use word embeddings***, which map every word in some vocabulary into a dense, fixed-length vector of numbers. 
  - To obtain this mapping, two methods exist.
    - First of all, you can download pretrained vectors for the language that you need. There are several sources of embeddings available; just search on Google for GloVe pretrained vectors or word2vec pretrained. (GloVe and word2vec are different methods used to train such vectors, which produce similar results.)
    - Another way to obtain embeddings is to train them on your own dataset. 

### The Encoder-Decoder architecture

Another model that is widely used in NLP is called Encoder-Decoder, or seq2seq.

The idea behind seq2seq is to use an RNN to process an input sequence and encode this sequence into some fixed-length representation. This RNN is called an encoder. Then you feed the encoded vector into another RNN, called a decoder, which has to produce the resulting sequence in the target language.

## Seq2seq training

The connection lies in the training process of the seq2seq model, but before we come to the modern RL approaches to the problem, I need to say a couple of words about the standard way of carrying out the training.

### Log-likelihood training

