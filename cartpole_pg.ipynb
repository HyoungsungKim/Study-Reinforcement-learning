{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bit66f7a8720ea44564891eb6b9b39e6c03",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'last_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0.04385597, -0.00612533,  0.04623462, -0.02609207])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGN(nn.Module):\n",
    "    def __init__(self, observation_space, n_actions):\n",
    "        super(PGN, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(observation_space.shape[0], 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, n_actions),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.state = self.env.reset()\n",
    "        self.next_state = None\n",
    "        self.total_reward_sum = []\n",
    "        self.total_reward_list = []        \n",
    "\n",
    "    def _predict_probs(self, states, net):\n",
    "        state = np.array(states).reshape(1, states.size)\n",
    "        state_v = torch.tensor(state, dtype=torch.float)\n",
    "        q_vals = net(state_v)\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        predict_prob = softmax(q_vals)\n",
    "\n",
    "        return predict_prob.data.numpy()\n",
    "\n",
    "    def play_step(self, net, step_size, device=\"cpu\"):\n",
    "        assert step_size >= 1, \"Too less step_size\"\n",
    "        print(\"Current state\",self.state)\n",
    "        reward_list = []\n",
    "        next_state_list = []\n",
    "        current_state = self.state  \n",
    "        cumulated_reward = None\n",
    "        first_step = True\n",
    "\n",
    "        for i in range(step_size):\n",
    "           # action_prob = self._predict_probs(self.state, net)[0]\n",
    "           # action = np.random.choice(len(action_prob), p=action_prob)\n",
    "            action = 1           \n",
    "            \n",
    "            new_state, reward, done, _ = self.env.step(action)\n",
    "            next_state_list.append(new_state)\n",
    "            print(next_state_list)\n",
    "            if first_step is True:\n",
    "                current_action = action\n",
    "                current_state = self.state\n",
    "\n",
    "                if done is True:\n",
    "                    self.env.reset()\n",
    "\n",
    "            self.state = new_state\n",
    "            reward_list.append(reward)\n",
    "            self.total_reward_list.append(reward)\n",
    "            \n",
    "            if done is True:\n",
    "                self.total_reward_sum.append(sum(self.total_reward_list))                    \n",
    "                new_state = None  \n",
    "                cumulated_reward = self._get_cumulated_reward(reward_list)\n",
    "                break\n",
    "\n",
    "        if cumulated_reward is None:\n",
    "            cumulated_reward = self._get_cumulated_reward(reward_list)\n",
    "\n",
    "        exp = Experience(current_state, current_action, cumulated_reward, new_state)           \n",
    "        self.state = next_state_list.pop(0)\n",
    "        print(\"Next state\", self.state)\n",
    "        return exp\n",
    "\n",
    "    def _get_cumulated_reward(self, rewards_list, gamma=1.0):\n",
    "        cumulated_reward = 0\n",
    "        for reward in rewards_list:\n",
    "            cumulated_reward *= gamma\n",
    "            cumulated_reward += reward\n",
    "\n",
    "        return cumulated_reward\n",
    "\n",
    "    def pop_total_rewards(self):\n",
    "        if not self.total_reward_sum:\n",
    "            #print(\"Empty\")\n",
    "            return self.total_reward_sum\n",
    "        else:\n",
    "            #print(\"Not empty\")\n",
    "            total_reward_sum = self.total_reward_sum\n",
    "            self.total_reward_sum = []\n",
    "            self.total_reward_list = []            \n",
    "            return total_reward_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(ToyEnv, self).__init__()\n",
    "        self.observation_space = gym.spaces.Discrete(n=5)\n",
    "        self.action_space = gym.spaces.Discrete(n=3)\n",
    "        self.step_index = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.step_index = 0\n",
    "        return self.step_index\n",
    "\n",
    "    def step(self, action):\n",
    "        is_done = self.step_index == 10\n",
    "        if is_done:\n",
    "            return self.step_index % self.observation_space.n, 0.0, is_done, {}\n",
    "        \n",
    "        self.step_index += 1\n",
    "        return self.step_index % self.observation_space.n, float(action), self.step_index == 10, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_env = gym.make(\"CartPole-v0\").env\n",
    "test_env = ToyEnv()\n",
    "test_env.reset()\n",
    "\n",
    "net = PGN(env.observation_space, env.action_space.n)\n",
    "agent = Agent(test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Current state 0\n[1]\n[1, 2]\nNext state 1\nExperience(state=1, action=1, reward=2.0, last_state=2)\nCurrent state 1\n[3]\n[3, 4]\nNext state 3\nExperience(state=3, action=1, reward=2.0, last_state=4)\nCurrent state 3\n[0]\n[0, 1]\nNext state 0\nExperience(state=0, action=1, reward=2.0, last_state=1)\nCurrent state 0\n[2]\n[2, 3]\nNext state 2\nExperience(state=2, action=1, reward=2.0, last_state=3)\nCurrent state 2\n[4]\n[4, 0]\nNext state 4\nExperience(state=4, action=1, reward=2.0, last_state=None)\nCurrent state 4\n[1]\n[1, 2]\nNext state 1\nExperience(state=1, action=1, reward=2.0, last_state=2)\nCurrent state 1\n[3]\n[3, 4]\nNext state 3\nExperience(state=3, action=1, reward=2.0, last_state=4)\nCurrent state 3\n[0]\n[0, 1]\nNext state 0\nExperience(state=0, action=1, reward=2.0, last_state=1)\nCurrent state 0\n[2]\n[2, 3]\nNext state 2\nExperience(state=2, action=1, reward=2.0, last_state=3)\nCurrent state 2\n[4]\n[4, 0]\nNext state 4\nExperience(state=4, action=1, reward=2.0, last_state=None)\n"
    }
   ],
   "source": [
    "for idx in range(10):\n",
    "    #print(agent.pop_total_rewards())\n",
    "    exp = agent.play_step(net, step_size=2)\n",
    "    print(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.001\n",
    "ENTROPY_BETA = 0.01\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0.04370282, -0.02918187, -0.04839809,  0.03518697])"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = PGN(env.observation_space, env.action_space.n)\n",
    "agent = Agent(env)\n",
    "done_episodes = 0\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "reward_sum = 0\n",
    "total_rewards = []\n",
    "batch_states, batch_actions, batch_scales = [], [], []\n",
    "frame_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "mean reward :  21.83\nmean reward :  22.95\nmean reward :  21.15\nmean reward :  21.76\nmean reward :  20.8\nmean reward :  21.06\nmean reward :  20.57\nmean reward :  21.3\nmean reward :  20.71\nmean reward :  24.2\nmean reward :  24.04\nmean reward :  20.94\nmean reward :  22.13\nmean reward :  24.29\nmean reward :  23.47\nmean reward :  23.22\nmean reward :  21.99\nmean reward :  21.33\nmean reward :  23.12\nmean reward :  22.37\nmean reward :  23.42\nmean reward :  21.29\nmean reward :  22.15\nmean reward :  22.42\nmean reward :  24.78\nmean reward :  24.4\nmean reward :  22.09\nmean reward :  24.51\nmean reward :  23.56\nmean reward :  19.73\nmean reward :  22.82\nmean reward :  21.24\nmean reward :  22.36\nmean reward :  23.55\nmean reward :  22.95\nmean reward :  22.68\nmean reward :  22.44\nmean reward :  21.8\nmean reward :  21.41\nmean reward :  22.8\nmean reward :  21.43\nmean reward :  20.55\nmean reward :  23.66\nmean reward :  23.38\nmean reward :  20.01\nmean reward :  24.72\nmean reward :  21.8\nmean reward :  22.5\nmean reward :  21.93\nmean reward :  22.14\nmean reward :  20.85\nmean reward :  22.96\nmean reward :  21.38\nmean reward :  20.17\nmean reward :  23.18\nmean reward :  20.35\nmean reward :  21.84\nmean reward :  20.8\nmean reward :  22.18\nmean reward :  22.15\nmean reward :  22.46\nmean reward :  19.85\nmean reward :  20.74\nmean reward :  24.2\nmean reward :  23.46\nmean reward :  20.52\nmean reward :  22.45\nmean reward :  23.55\nmean reward :  20.25\nmean reward :  22.76\nmean reward :  22.49\nmean reward :  23.13\nmean reward :  22.86\nmean reward :  23.28\nmean reward :  22.09\nmean reward :  23.32\nmean reward :  21.61\nmean reward :  21.07\nmean reward :  25.25\nmean reward :  22.39\nmean reward :  20.84\nmean reward :  24.03\nmean reward :  21.07\nmean reward :  21.74\nmean reward :  21.04\nmean reward :  20.58\nmean reward :  22.66\nmean reward :  20.38\nmean reward :  21.07\nmean reward :  21.82\nmean reward :  21.05\nmean reward :  23.21\nmean reward :  22.98\nmean reward :  22.77\nmean reward :  22.23\nmean reward :  22.94\nmean reward :  22.3\nmean reward :  22.13\nmean reward :  22.63\nmean reward :  22.61\nmean reward :  19.64\nmean reward :  22.34\nmean reward :  24.04\nmean reward :  21.33\nmean reward :  19.96\nmean reward :  22.38\nmean reward :  22.24\nmean reward :  21.38\nmean reward :  21.62\nmean reward :  21.92\nmean reward :  21.02\nmean reward :  22.07\nmean reward :  20.56\nmean reward :  20.31\nmean reward :  23.44\nmean reward :  23.23\nmean reward :  21.58\nmean reward :  21.77\nmean reward :  21.0\nmean reward :  22.81\nmean reward :  21.62\nmean reward :  22.06\nmean reward :  20.47\nmean reward :  22.96\nmean reward :  23.48\nmean reward :  24.09\nmean reward :  20.75\nmean reward :  22.48\nmean reward :  22.55\nmean reward :  24.06\nmean reward :  23.2\nmean reward :  25.99\nmean reward :  22.09\nmean reward :  21.65\nmean reward :  24.17\nmean reward :  23.08\nmean reward :  23.73\nmean reward :  21.64\nmean reward :  22.4\nmean reward :  20.84\nmean reward :  20.71\n"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-4195759a3763>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# * loss + (-entropy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mloss_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_policy_v\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mentropy_loss_v\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mloss_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:              \n",
    "    exp = agent.play_step(net, step_size=1)    \n",
    "    reward_sum += exp.reward\n",
    "    baseline = reward_sum / (frame_idx + 1)\n",
    "    frame_idx += 1\n",
    "\n",
    "    batch_states.append(exp.state)\n",
    "    batch_actions.append(int(exp.action))\n",
    "    batch_scales.append(exp.reward - baseline)\n",
    "\n",
    "    new_rewards = agent.pop_total_rewards()    \n",
    "    if new_rewards:\n",
    "        done_episodes += 1\n",
    "        reward = new_rewards[0]\n",
    "        total_rewards.append(reward)\n",
    "        mean_reward = float(np.mean(total_rewards[-100:]))\n",
    "\n",
    "        if done_episodes % 100 == 0:\n",
    "            print(\"mean reward : \", mean_reward)\n",
    "\n",
    "        if mean_reward > 200:\n",
    "            print(\"WIN!\")\n",
    "            break\n",
    "\n",
    "    if len(batch_states) < BATCH_SIZE:\n",
    "        continue\n",
    "\n",
    "\n",
    "    states_v = torch.tensor(batch_states, dtype=torch.float)\n",
    "    batch_actions_t = torch.tensor(batch_actions, dtype=torch.long)\n",
    "    batch_scale_v = torch.tensor(batch_scales, dtype=torch.float)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    logits_v = net(states_v)\n",
    "    log_softmax = nn.LogSoftmax(dim=1)\n",
    "    log_prob_v = log_softmax(logits_v)\n",
    "    log_prob_actions_v = batch_scale_v * log_prob_v[range(len(batch_states)), batch_actions_t]\n",
    "    # log_prob_actions_v = log_prob_v[range(BATCH_SIZE), batch_actions_t]\n",
    "    loss_policy_v = -log_prob_actions_v.mean()\n",
    "\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    prob_v = softmax(logits_v)\n",
    "    entropy_v = -(prob_v * log_prob_v).sum(dim=1).mean()\n",
    "    entropy_loss_v = -ENTROPY_BETA * entropy_v\n",
    "    # * loss + (-entropy)        \n",
    "    loss_v = loss_policy_v + entropy_loss_v\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    batch_states.clear()\n",
    "    batch_actions.clear()\n",
    "    batch_scales.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True)\n",
    "\n",
    "states, actions, rewards = agent.play_step(net)\n",
    "                           \n",
    "sessions = [agent.play_step(net) for _ in range(5)]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0\n"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-a657afe025bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;34m<\u001b[0m\u001b[0msource\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"{}\"\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"video/mp4\"\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \"\"\".format(\"./videos/\"+video_names[0]))  # this may or may not be the _last_ video. Try other indices\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n",
    "\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[0]))  # this may or may not be the _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}