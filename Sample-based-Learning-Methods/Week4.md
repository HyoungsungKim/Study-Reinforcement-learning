# Chapter 8 Planning and Learning with Tabular Methods 

- Sample model : It requires less memory
  - ***Can only approximate*** this expected outcome by averaging many samples together, knowing the exact probabilities also has the flexibility of assessing risk
- Distribution model : It can be used to ***compute the exact expected outcome*** by summing over all outcomes weighted by their probabilities

- ***Model-based*** methods rely on ***planning*** as their primary component
- ***model-free*** methods primarily rely on ***learning***.

Although there are real differences between these two kinds of methods, there are also great similarities. In particular, ***the heart of both kinds of methods is the computation of value functions.*** Moreover, all the methods are based on looking ahead to future events, computing a backed-up value, and then using it as an update target for an approximate value function. 

## 8.1 Models and Planning

By a ***model*** of the environment we mean anything that an agent can use to predict how the environment will respond to its actions. ***Given a state and an action, a model produces a prediction of the resultant next state and next reward.***

If the model is stochastic, then there are several possible next states and next rewards, each with some probability of occurring.

- ***Some models produce a description of all possibilities and their probabilities; these we call distribution models.***
- ***Other models produce just one of the possibilities, sampled according to the probabilities; these we call sample models.***
  - For example, consider modeling the sum of a dozen dice. A distribution model would produce all possible sums and their probabilities of occurring
  - Whereas a sample model would produce an individual sum drawn according to this probability distribution.

Models can be used to mimic or simulate experience. Given a starting state and action,

- A sample model produces a possible transition
- Distribution model generates all possible transitions weighted by their probabilities of occurring.
  - Given a starting state and a policy, a sample model could produce an entire episode, and a distribution model could generate all possible episodes and their probabilities. In either case, we say the model is used to simulate the environment and produce simulated experience.

The word ***planning*** is used in several different ways in different fields. We use the term to refer to ***any computational process*** that takes a model as input and produces or improves a policy for interacting with the modeled environment:
$$
model \xrightarrow{planning} policy
$$
In artificial intelligence, there are two distinct approaches to planning according to our definition.

- ***State-space planning*** is viewed primarily as a search through the state space for an optimal policy or an optimal path to a goal.
  - Actions cause transitions from state to state, and value functions are computed over states.
- In what we call ***plan-space planning***, planning is instead a search through the space of plans. Operators transform one plan into another, and value functions, if any, are defined over the space of plans.
  - Plan-space planning includes evolutionary methods and “partial-order planning,” a common kind of planning in artificial intelligence in which the ordering of steps is not completely determined at all stages of planning.
  - Plan-space methods are difficult to apply efficiently to the stochastic sequential decision problems that are the focus in reinforcement learning, and we do not consider them further.

The heart of both learning and planning methods is the estimation of value functions by backing-up update operations. The difference is 

- ***Planning uses simulated experience*** generated by a model
- ***Learning methods use real experience*** generated by the environment.

***But the common structure means that many ideas and algorithms can be transferred between planning and learning.*** In particular, in many cases a learning algorithm can be substituted for the key update step of a planning method. Learning methods require only experience as input, and in many cases they can be applied to simulated experience just as well as to real experience.