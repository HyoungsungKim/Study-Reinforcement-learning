# Chapter 8 Planning and Learning with Tabular Methods 

- Sample model : It requires less memory
  - ***Can only approximate*** this expected outcome by averaging many samples together, knowing the exact probabilities also has the flexibility of assessing risk
- Distribution model : It can be used to ***compute the exact expected outcome*** by summing over all outcomes weighted by their probabilities

- ***Model-based*** methods rely on ***planning*** as their primary component
- ***model-free*** methods primarily rely on ***learning***.

Although there are real differences between these two kinds of methods, there are also great similarities. In particular, ***the heart of both kinds of methods is the computation of value functions.*** Moreover, all the methods are based on looking ahead to future events, computing a backed-up value, and then using it as an update target for an approximate value function. 

## 8.1 Models and Planning

By a ***model*** of the environment we mean anything that an agent can use to predict how the environment will respond to its actions. ***Given a state and an action, a model produces a prediction of the resultant next state and next reward.***

If the model is stochastic, then there are several possible next states and next rewards, each with some probability of occurring.

- ***Some models produce a description of all possibilities and their probabilities; these we call distribution models.***
- ***Other models produce just one of the possibilities, sampled according to the probabilities; these we call sample models.***
  - For example, consider modeling the sum of a dozen dice. A distribution model would produce all possible sums and their probabilities of occurring
  - Whereas a sample model would produce an individual sum drawn according to this probability distribution.

Models can be used to mimic or simulate experience. Given a starting state and action,

- A sample model produces a possible transition
- Distribution model generates all possible transitions weighted by their probabilities of occurring.
  - Given a starting state and a policy, a sample model could produce an entire episode, and a distribution model could generate all possible episodes and their probabilities. In either case, we say the model is used to simulate the environment and produce simulated experience.

The word ***planning*** is used in several different ways in different fields. We use the term to refer to ***any computational process*** that takes a model as input and produces or improves a policy for interacting with the modeled environment:
$$
model \xrightarrow{planning} policy
$$
In artificial intelligence, there are two distinct approaches to planning according to our definition.

- ***State-space planning*** is viewed primarily as a search through the state space for an optimal policy or an optimal path to a goal.
  - Actions cause transitions from state to state, and value functions are computed over states.
- In what we call ***plan-space planning***, planning is instead a search through the space of plans. Operators transform one plan into another, and value functions, if any, are defined over the space of plans.
  - Plan-space planning includes evolutionary methods and “partial-order planning,” a common kind of planning in artificial intelligence in which the ordering of steps is not completely determined at all stages of planning.
  - Plan-space methods are difficult to apply efficiently to the stochastic sequential decision problems that are the focus in reinforcement learning, and we do not consider them further.

The heart of both learning and planning methods is the estimation of value functions by backing-up update operations. The difference is 

- ***Planning uses simulated experience*** generated by a model
- ***Learning methods use real experience*** generated by the environment.

***But the common structure means that many ideas and algorithms can be transferred between planning and learning.*** In particular, in many cases a learning algorithm can be substituted for the key update step of a planning method. Learning methods require only experience as input, and in many cases they can be applied to simulated experience just as well as to real experience.

## 8.2 Dyna: Integrated Planning, Acting, and Learning

When planning is done online, while interacting with the environment, ***a number of interesting issues arise***.

- New information gained from the interaction may change the model and thereby interact with planning.
  - Interaction으로 얻은 새로운 정보는 model을 바꿀 가능성이 있고, 이는 planning에도 영향을 준다.
- It may be desirable to customize the planning process in some way to the states or decisions currently under consideration, or expected in the near future.
  - Planning process를 현재 또는 가까운 미래에 고려 중인 state 또는 decision으로 수정하는 것이 바람직하다.
- If decision making and model learning are both computation-intensive processes, then the available computational resources may need to be divided between them.
  - 만약 decision making and model learning이 계산 중심 과정(Computation-intensive processes)이라면 사용 가능한 계산 자원은 이것들(decision making and model learning)에 의해 나누어 질 것 이다.
- ***To begin exploring these issues, in this section we present Dyna-Q***, a simple architecture integrating the major functions needed in an online planning agent.

***Within a planning agent, there are at least two roles for real experience:*** it can be used to improve the model (to make it more accurately match the real environment) and it can be used to directly improve the value function and policy using the kinds of reinforcement learning methods we have discussed in previous chapters.

- Agent를 planning안에서, real experience를 위한 적어도 두가지 역할이 있다. 
  - 현실 경험과 더욱 정확하게 매칭 되기 위해 모델을 향상 시킴(model-learning)
  - value function과 policy 향상에 사용됨(reinforcement learning)

The former we call model-learning(indirect), and the latter we call direct reinforcement learning (direct RL).

Both direct and indirect methods have advantages and disadvantages.

- Indirect methods often make fuller use of a limited amount of experience and thus achieve a better policy with fewer environmental interactions.
- On the other hand, direct methods are much simpler and are not affected by biases in the design of the model.

Dyna-Q includes all of the processes shown in the diagram above—planning, acting, model-learning, and direct RL—all occurring continually. The planning method is the random-sample one-step tabular Q-planning method on page 161. The direct RL method is one-step tabular Q-learning. The model-learning method is also table-based and assumes the environment is deterministic. After each transition $$S_t , A_t \to R_{t+1} , S_{t+1}$$, the model records in its table entry for $$S_t , A_t$$ the prediction that $$R_{t+1} , S_{t+1}$$ will deterministically follow.

- Thus, ***if the model is queried with a state–action pair that has been experienced before, it simply returns the last-observed next state and next reward as its prediction.***
  - 테이블이 있기 때문에 테이블에 기록된 값 return 하면 됨
- During planning, the Q-planning algorithm randomly samples only from state–action pairs that have previously been experienced (in Step 1), so the model is never queried with a pair about which it has no information.
  - Planning에서는 이전에 경험 했던 값을에 대해서만 query 함

Typically, as in Dyna-Q, the ***same reinforcement learning method is used both for learning from real experience and for planning from simulated experience***.

- The reinforcement learning method is thus the “final common path” for both learning and planning.
- Learning and planning are deeply integrated in the sense that they share almost all the same machinery, differing only in the source of their experience.

***Conceptually, planning, acting, model-learning, and direct RL occur simultaneously and in parallel in Dyna agents.*** For concreteness and implementation on a serial computer, however, ***we fully specify the order in which they occur within a time step.***

- Dyna agent는 parallel하게 동시에 일어나지만, serial 컴퓨터에서 동작하기 위해서 순서(Order)를 정해야 함.

***In Dyna-Q, the acting, model-learning, and direct RL processes require little computation***, and we assume they consume just a fraction of the time. The remaining time in each step can be devoted to the ***planning process, which is inherently computation-intensive***.

In Dyna-Q, learning and planning are accomplished by exactly the same algorithm, operating on real experience for learning and on simulated experience for planning. Because planning proceeds incrementally, it is trivial to intermix planning and acting. Both proceed as fast as they can. The agent is always reactive and always deliberative, responding instantly to the latest sensory information and yet always planning in the background. Also ongoing in the background is the model-learning process. As new information is gained, the model is updated to better match reality. As the model changes, the ongoing planning process will gradually compute a different way of behaving to match the new model.